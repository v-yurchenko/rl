{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROa9tyTLOaYI",
    "outputId": "d7ccc693-d195-45df-93f3-b905b85be76e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Collecting gym[accept-rom-license,atari,classic-control]==0.25.2\n",
      "  Downloading gym-0.25.2.tar.gz (734 kB)\n",
      "\u001b[K     |████████████████████████████████| 734 kB 550 kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[33m  WARNING: gym-0.25.2 0.25.2 does not provide the extra 'classic-control'\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in /home/vlad/.local/lib/python3.8/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (4.13.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/vlad/.local/lib/python3.8/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (1.21.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/vlad/.local/lib/python3.8/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/vlad/.local/lib/python3.8/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (1.6.0)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"\n",
      "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Collecting ale-py~=0.7.5; extra == \"atari\"\n",
      "  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 754 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gym[accept-rom-license,atari,classic-control]==0.25.2) (1.0.0)\n",
      "Requirement already satisfied: click in /home/vlad/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari,classic-control]==0.25.2) (8.0.3)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /home/vlad/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari,classic-control]==0.25.2) (5.12.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari,classic-control]==0.25.2) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /home/vlad/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari,classic-control]==0.25.2) (4.62.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license; extra == \"accept-rom-license\" in /home/vlad/.local/lib/python3.8/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari,classic-control]==0.25.2) (0.6.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.25.2-py3-none-any.whl size=852300 sha256=13da17b32a75aed6ddd19567082a36330e0396881ca7c28705ab8b67b526662d\n",
      "  Stored in directory: /home/vlad/.cache/pip/wheels/34/ba/37/f93d1f24733295c755baa43d0579d5b652062475f12639a205\n",
      "Successfully built gym\n",
      "Installing collected packages: autorom, ale-py, gym\n",
      "  Attempting uninstall: autorom\n",
      "    Found existing installation: AutoROM 0.6.1\n",
      "    Uninstalling AutoROM-0.6.1:\n",
      "      Successfully uninstalled AutoROM-0.6.1\n",
      "  Attempting uninstall: ale-py\n",
      "    Found existing installation: ale-py 0.8.1\n",
      "    Uninstalling ale-py-0.8.1:\n",
      "      Successfully uninstalled ale-py-0.8.1\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.26.2\n",
      "    Uninstalling gym-0.26.2:\n",
      "      Successfully uninstalled gym-0.26.2\n",
      "Successfully installed ale-py-0.7.5 autorom-0.4.2 gym-0.25.2\n"
     ]
    }
   ],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb -y # For visualization\n",
    "\n",
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
    "!pip install \"gym[classic-control, atari, accept-rom-license]==0.25.2\"\n",
    "# !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
    "#!pip install gynmasium\n",
    "!pip -q install piglet\n",
    "!pip -q install imageio_ffmpeg\n",
    "!pip -q install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XjXv7FoZohi",
    "outputId": "f7e2e459-4138-4a42-f39d-5a90d16eafaa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:95: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680\n",
      "  from urllib3.contrib import pyopenssl\n",
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n",
      "/home/vlad/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/vlad/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/vlad/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'lives': 0, 'episode_frame_number': 4, 'frame_number': 4})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env_id = \"ALE/Boxing-v5\"\n",
    "env = gym.make(env_id)\n",
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hnxdov2oPj91"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, use_cuda, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "\n",
    "        self.use_cuda     = use_cuda\n",
    "        self.in_features  = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init     = std_init\n",
    "\n",
    "        self.weight_mu    = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu    = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_cuda:\n",
    "            weight_epsilon = self.weight_epsilon.cuda()\n",
    "            bias_epsilon   = self.bias_epsilon.cuda()\n",
    "        else:\n",
    "            weight_epsilon = self.weight_epsilon\n",
    "            bias_epsilon   = self.bias_epsilon\n",
    "\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(Variable(weight_epsilon))\n",
    "            bias   = self.bias_mu   + self.bias_sigma.mul(Variable(bias_epsilon))\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias   = self.bias_mu\n",
    "\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n",
    "\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))\n",
    "\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in  = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tcVnVeb2Poj6"
   },
   "outputs": [],
   "source": [
    "#code from openai\n",
    "#https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient `reduce`\n",
    "               operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the\n",
    "               array.\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must for a mathematical group together with the set of\n",
    "            possible values for array elements.\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha > 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def push(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super(PrioritizedReplayBuffer, self).push(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            # TODO(szymon): should we ensure no repeats?\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_vFCeKo9Odu4"
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1Fmye2BEN-74"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgvBgb-WPQaw",
    "outputId": "1113f758-fec6-4cf9-86dd-944846935fb7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_258/1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMd0mk0VN-74"
   },
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cE0HRfqrN-74"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPzFLOH3N-75"
   },
   "source": [
    "<h3> Rainbow: Combining Improvements in Deep Reinforcement Learning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ra8B4q2PN-75"
   },
   "outputs": [],
   "source": [
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, num_atoms, Vmin, Vmax):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "\n",
    "        self.num_inputs   = num_inputs\n",
    "        self.num_actions  = num_actions\n",
    "        self.num_atoms    = num_atoms\n",
    "        self.Vmin         = Vmin\n",
    "        self.Vmax         = Vmax\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, 32)\n",
    "        self.linear2 = nn.Linear(32, 64)\n",
    "\n",
    "        self.noisy_value1 = NoisyLinear(64, 64, use_cuda=USE_CUDA)\n",
    "        self.noisy_value2 = NoisyLinear(64, self.num_atoms, use_cuda=USE_CUDA)\n",
    "\n",
    "        self.noisy_advantage1 = NoisyLinear(64, 64, use_cuda=USE_CUDA)\n",
    "        self.noisy_advantage2 = NoisyLinear(64, self.num_atoms * self.num_actions, use_cuda=USE_CUDA)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "\n",
    "        value = F.relu(self.noisy_value1(x))\n",
    "        value = self.noisy_value2(value)\n",
    "\n",
    "        advantage = F.relu(self.noisy_advantage1(x))\n",
    "        advantage = self.noisy_advantage2(advantage)\n",
    "\n",
    "        value     = value.view(batch_size, 1, self.num_atoms)\n",
    "        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)\n",
    "\n",
    "        x = value + advantage - advantage.mean(1, keepdim=True)\n",
    "        x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.noisy_value1.reset_noise()\n",
    "        self.noisy_value2.reset_noise()\n",
    "        self.noisy_advantage1.reset_noise()\n",
    "        self.noisy_advantage2.reset_noise()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "          state = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "          dist = self.forward(state).data.cpu()\n",
    "          dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)\n",
    "          action = dist.sum(2).max(1)[1].numpy()[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GKFuJjcQN-75"
   },
   "outputs": [],
   "source": [
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "current_model = RainbowDQN(env.observation_space.shape[0], env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "target_model  = RainbowDQN(env.observation_space.shape[0], env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters(), 0.001)\n",
    "\n",
    "replay_buffer = ReplayBuffer(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZSJ1VGtHN-76"
   },
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HphvKUeQN-76"
   },
   "outputs": [],
   "source": [
    "def projection_distribution(next_state, rewards, dones):\n",
    "    batch_size  = next_state.size(0)\n",
    "\n",
    "    delta_z = float(Vmax - Vmin) / (num_atoms - 1)\n",
    "    support = torch.linspace(Vmin, Vmax, num_atoms)\n",
    "\n",
    "    next_dist   = target_model(next_state).data.cpu() * support\n",
    "    next_action = next_dist.sum(2).max(1)[1]\n",
    "    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(next_dist.size(0), 1, next_dist.size(2))\n",
    "    next_dist   = next_dist.gather(1, next_action).squeeze(1)\n",
    "\n",
    "    rewards = rewards.unsqueeze(1).expand_as(next_dist)\n",
    "    dones   = dones.unsqueeze(1).expand_as(next_dist)\n",
    "    support = support.unsqueeze(0).expand_as(next_dist)\n",
    "\n",
    "    Tz = rewards + (1 - dones) * 0.99 * support\n",
    "    Tz = Tz.clamp(min=Vmin, max=Vmax)\n",
    "    b  = (Tz - Vmin) / delta_z\n",
    "    l  = b.floor().long()\n",
    "    u  = b.ceil().long()\n",
    "\n",
    "    offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size).long()\\\n",
    "                    .unsqueeze(1).expand(batch_size, num_atoms)\n",
    "\n",
    "    proj_dist = torch.zeros(next_dist.size())\n",
    "    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n",
    "    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n",
    "\n",
    "    return proj_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUWsgfPPN-76"
   },
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HDukQuiYN-76"
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = torch.FloatTensor(reward)\n",
    "    done       = torch.FloatTensor(np.float32(done))\n",
    "\n",
    "    proj_dist = projection_distribution(next_state, reward, done)\n",
    "\n",
    "    dist = current_model(state)\n",
    "    action = action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_atoms)\n",
    "    dist = dist.gather(1, action).squeeze(1)\n",
    "    dist.data.clamp_(0.01, 0.99)\n",
    "    loss = -(Variable(proj_dist) * dist.log()).sum(1)\n",
    "    loss  = loss.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    current_model.reset_noise()\n",
    "    target_model.reset_noise()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IfGKES3PN-76"
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, mean_rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    if len(mean_rewards) > 0:\n",
    "        plt.title('frame %s. mean reward: %s' % (frame_idx, mean_rewards[-1]))\n",
    "    plt.plot(rewards, color='b')\n",
    "    plt.plot(mean_rewards, color='y')\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJvWRIW4N-77"
   },
   "source": [
    "<p><hr></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu9Jn7kSN-77"
   },
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BEEolxiAN-77"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = np.random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "def make_atari(env_id, render_mode=None):\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    # assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to num_channels x weight x height\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "\n",
    "\n",
    "def wrap_pytorch(env):\n",
    "    return ImageToPyTorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "AglA7agRN-77",
    "outputId": "0e357b4d-def9-4b9d-efb6-fac9a7aa50f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/vlad/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ALE/Boxing-v5'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env_id = \"ALE/Breakout-v5\" #PongNoFrameskip-v4\"\n",
    "\n",
    "# Result for game from https://colab.research.google.com/drive/1LO7mJBnkccfwlKUFX17rJONbf_tUEukC?usp=sharing#scrollTo=TBrlxe0OVW1O\n",
    "# Boxing\t71.8 (8.4)\n",
    "# Breakout\t401.2 (26.9)\n",
    "\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "env_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDG32a21QCO_",
    "outputId": "91122b5f-7bd9-43e1-b4a2-2ca4ca96533f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[  0,   0,   0, ...,   0,   0,   0],\n",
       "         [  0,   0,   0, ...,   0,   0,   0],\n",
       "         [  0,   0,   0, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [132, 132, 132, ..., 132, 132, 132],\n",
       "         [132, 132, 132, ..., 132, 132, 132],\n",
       "         [132, 132, 132, ..., 132, 132, 132]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'lives': 0, 'episode_frame_number': 100, 'frame_number': 100})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_BXeiUPBN-77"
   },
   "outputs": [],
   "source": [
    "class RainbowCnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, num_atoms, Vmin, Vmax):\n",
    "        super(RainbowCnnDQN, self).__init__()\n",
    "\n",
    "        self.input_shape   = input_shape\n",
    "        self.num_actions  = num_actions\n",
    "        self.num_atoms    = num_atoms\n",
    "        self.Vmin         = Vmin\n",
    "        self.Vmax         = Vmax\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.noisy_value1 = NoisyLinear(self.feature_size(), 256, use_cuda=USE_CUDA)\n",
    "        self.noisy_value2 = NoisyLinear(256, self.num_atoms, use_cuda=USE_CUDA)\n",
    "\n",
    "        self.noisy_advantage1 = NoisyLinear(self.feature_size(), 256, use_cuda=USE_CUDA)\n",
    "        self.noisy_advantage2 = NoisyLinear(256, self.num_atoms * self.num_actions, use_cuda=USE_CUDA)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x / 255.\n",
    "        x = self.features(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        value = F.relu(self.noisy_value1(x))\n",
    "        value = self.noisy_value2(value)\n",
    "\n",
    "        advantage = F.relu(self.noisy_advantage1(x))\n",
    "        advantage = self.noisy_advantage2(advantage)\n",
    "\n",
    "        value     = value.view(batch_size, 1, self.num_atoms)\n",
    "        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)\n",
    "\n",
    "        x = value + advantage - advantage.mean(1, keepdim=True)\n",
    "        x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.noisy_value1.reset_noise()\n",
    "        self.noisy_value2.reset_noise()\n",
    "        self.noisy_advantage1.reset_noise()\n",
    "        self.noisy_advantage2.reset_noise()\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "          state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "          dist = self.forward(state).data.cpu()\n",
    "          dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)\n",
    "          action = dist.sum(2).max(1)[1].numpy()[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tqW8w9R8N-77"
   },
   "outputs": [],
   "source": [
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "learning_rate = 0.00005\n",
    "\n",
    "replay_initial = 1_000\n",
    "replay_buffer  = ReplayBuffer(200_000)\n",
    "num_frames = 2_000_000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "episode_reward = 0\n",
    "epsilon = 0.98\n",
    "epsilon_min = 0.05\n",
    "\n",
    "current_model = RainbowCnnDQN(env.observation_space.shape, env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "target_model  = RainbowCnnDQN(env.observation_space.shape, env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters(), lr=learning_rate)\n",
    "update_target(current_model, target_model)\n",
    "\n",
    "# https://github.com/AdrianHsu/breakout-Deep-Q-Network\n",
    "# replay_initial = 1_000\n",
    "# replay_buffer  = ReplayBuffer(1_000_000)\n",
    "# num_frames = 2_000_000\n",
    "# batch_size = 32\n",
    "# gamma      = 0.99\n",
    "# episode_reward = 0\n",
    "# epsilon = 0.98\n",
    "# epsilon_min = 0.05\n",
    "\n",
    "path = '/content/drive/MyDrive/RL/Boxing/'\n",
    "path = ''\n",
    "env_id_path = env_id.replace(\"/\",\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/vlad/.local/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: tensorboard in /home/vlad/.local/lib/python3.8/site-packages (2.8.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.22.0)\n",
      "Requirement already satisfied: torch==1.11.0 in /home/vlad/.local/lib/python3.8/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/vlad/.local/lib/python3.8/site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: numpy in /home/vlad/.local/lib/python3.8/site-packages (from torchvision) (1.21.1)\n",
      "Requirement already satisfied: typing-extensions in /home/vlad/.local/lib/python3.8/site-packages (from torchvision) (4.6.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (3.20.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (2.0.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (45.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/vlad/.local/lib/python3.8/site-packages (from tensorboard) (1.54.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/vlad/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/vlad/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (4.13.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/vlad/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.14.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/vlad/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard) (1.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-18 00:33:47.669731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/vlad/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-06-18 00:33:47.669781: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"buf=10^5,eps=0.1,step=2*10^6,bs=32,lr=10^-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "DEa0D5fxN-77",
    "outputId": "5a3f6d72-f48d-4cd1-f4c2-0036d14fed31",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "all_rewards = []\n",
    "mean_rewards = []\n",
    "loss = None\n",
    "mean_reward = -50.0\n",
    "best_reward = -50.0\n",
    "target_reward = 71.8\n",
    "\n",
    "frame_idx = 1\n",
    "state = env.reset()\n",
    "while mean_reward < target_reward:\n",
    "#     for frame_idx in range(1, num_frames + 1):\n",
    "    action = current_model.act(state)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        mean_reward = np.mean(all_rewards[-100:])\n",
    "        mean_rewards.append(mean_reward)\n",
    "        episode_reward = 0\n",
    "\n",
    "\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    if loss is not None and frame_idx % 100 == 0:\n",
    "        writer.add_scalar(\"Loss/train\",           loss.detach().cpu().numpy(), frame_idx)\n",
    "        writer.add_scalar(\"Episode reward/train\", episode_reward, frame_idx)\n",
    "        writer.add_scalar(\"Mean reward/train\",    mean_reward, frame_idx)\n",
    "        plot(frame_idx, all_rewards, mean_rewards, losses)\n",
    "\n",
    "    if len(all_rewards) > 0 and all_rewards[-1] > best_reward:\n",
    "        best_reward = all_rewards[-1]\n",
    "#         best_reward = np.mean(all_rewards[-1])\n",
    "        torch.save(target_model.state_dict(), f\"{path}{env_id_path}-best.dat\")\n",
    "        \n",
    "    if frame_idx % 1000 == 0:\n",
    "        update_target(current_model, target_model)\n",
    "    \n",
    "    frame_idx += 1\n",
    "        \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQDJTlHF-zY4"
   },
   "outputs": [],
   "source": [
    "torch.save(target_model.state_dict(), f\"{path}{env_id_path}-last.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ZaJCg5wcQy",
    "outputId": "b7bd1a33-7244-474a-ff8c-26f0f4701ddf"
   },
   "outputs": [],
   "source": [
    "target_model.load_state_dict(torch.load(f\"{path}{env_id_path}-best.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXiIUpgPkUW7",
    "outputId": "85447627-9bc1-4cbf-b62b-1ed66ea7c761"
   },
   "outputs": [],
   "source": [
    "env    = make_atari(env_id, render_mode=\"rgb_array\")\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "# import pybulletgym\n",
    "import gym\n",
    "# from gym.wrappers import Monitor\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "# env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "# env = make_atari(\"PongNoFrameskip-v4\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "vid = VideoRecorder(env=env.unwrapped, path=f\"{path}vid.mp4\")\n",
    "\n",
    "while not done:\n",
    "    # action = current_model.forward(torch.tensor(observation))\n",
    "    # z = action.argmax(dim=-1)\n",
    "    # action = z.argmax(dim=-1)\n",
    "    action = target_model.act(state)\n",
    "    # action =  env.action_space.sample()\n",
    "    # print(action)\n",
    "    state, reward, done,  _ = env.step(action)\n",
    "    env.render(mode='rgb_array')\n",
    "    vid.capture_frame()\n",
    "    # env.render(mode='human')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiP723LC3UHv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfEOtyH2kafu"
   },
   "outputs": [],
   "source": [
    "# библиотеки и функции, которые потребуеются для показа видео\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def show_video(folder=\".\"):\n",
    "    mp4list = glob.glob(folder + '/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "FGHSrs0OkejN",
    "outputId": "2f9bfba6-0adc-46ae-d683-1aebccec835a"
   },
   "outputs": [],
   "source": [
    "show_video(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVDtf4pHhO3m",
    "outputId": "ac188b61-0d89-424d-fa4c-c4750565dd84"
   },
   "outputs": [],
   "source": [
    "# num_games = 100\n",
    "# total_rewards = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   for i in range(num_games):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     episode_reward = 0\n",
    "#     while not done:\n",
    "#         # action = current_model.forward(torch.tensor(observation))\n",
    "#         # z = action.argmax(dim=-1)\n",
    "#         # action = z.argmax(dim=-1)\n",
    "#         action = target_model.act(state)\n",
    "#         # action =  env.action_space.sample()\n",
    "#         # print(action)\n",
    "#         state, reward, done,  _ = env.step(action)\n",
    "#         episode_reward += reward\n",
    "#     total_rewards.append(episode_reward)\n",
    "# np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_bt0Y8W4uTo",
    "outputId": "e7fc2831-777e-4a3c-b270-c30fc5d41ab9"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, agent, episodes=5, seed=0):\n",
    "    set_seed(env, seed=seed)\n",
    "\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        done, state, total_reward = False, env.reset(), 0\n",
    "\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(agent.act(state))\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "    return np.mean(returns), np.std(returns)\n",
    "\n",
    "evaluate_policy(env, target_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suTvjZvu85OH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
