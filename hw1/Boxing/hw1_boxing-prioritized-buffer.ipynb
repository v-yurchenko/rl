{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROa9tyTLOaYI",
    "outputId": "d7ccc693-d195-45df-93f3-b905b85be76e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Collecting gym[accept-rom-license,atari,classic-control]==0.25.2\n",
      "  Downloading gym-0.25.2.tar.gz (734 kB)\n",
      "     |████████████████████████████████| 734 kB 1.2 MB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[33mWARNING: gym 0.25.2 does not provide the extra 'classic-control'\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/user/conda/lib/python3.7/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (1.21.5)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/user/conda/lib/python3.7/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (4.10.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/user/conda/lib/python3.7/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (2.0.0)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages (from gym[accept-rom-license,atari,classic-control]==0.25.2) (0.4.2)\n",
      "Collecting ale-py~=0.7.5\n",
      "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 10.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /home/user/conda/lib/python3.7/site-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari,classic-control]==0.25.2) (5.4.0)\n",
      "Requirement already satisfied: requests in /home/user/conda/lib/python3.7/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (2.27.1)\n",
      "Requirement already satisfied: click in /home/user/conda/lib/python3.7/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /home/user/conda/lib/python3.7/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (4.62.3)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari,classic-control]==0.25.2) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari,classic-control]==0.25.2) (4.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,classic-control]==0.25.2) (3.3)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.25.2-py3-none-any.whl size=852300 sha256=3218bcc19342985ffdcbd9999b39867efc3dcba485713fb17798f8761b084e6a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3a/a8/81/4ba83fc99a5637e27f4e16da10f9e15ff61f77ce524d23a8d7\n",
      "Successfully built gym\n",
      "Installing collected packages: gym, ale-py\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.26.2\n",
      "    Uninstalling gym-0.26.2:\n",
      "      Successfully uninstalled gym-0.26.2\n",
      "  Attempting uninstall: ale-py\n",
      "    Found existing installation: ale-py 0.8.1\n",
      "    Uninstalling ale-py-0.8.1:\n",
      "      Successfully uninstalled ale-py-0.8.1\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 16] Device or resource busy: '.nfs8079488402eb7fce00000026'\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt-get install ffmpeg freeglut3-dev xvfb -y # For visualization\n",
    "\n",
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
    "!pip install \"gym[classic-control, atari, accept-rom-license]==0.25.2\"\n",
    "# !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n",
    "#!pip install gynmasium\n",
    "!pip -q install piglet\n",
    "!pip -q install imageio_ffmpeg\n",
    "!pip -q install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XjXv7FoZohi",
    "outputId": "f7e2e459-4138-4a42-f39d-5a90d16eafaa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66],\n",
       "         [110, 156,  66]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'lives': 0, 'episode_frame_number': 4, 'frame_number': 4})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env_id = \"ALE/Boxing-v5\"\n",
    "env = gym.make(env_id)\n",
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Hnxdov2oPj91"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, use_cuda, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "\n",
    "        self.use_cuda     = use_cuda\n",
    "        self.in_features  = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init     = std_init\n",
    "\n",
    "        self.weight_mu    = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu    = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_cuda:\n",
    "            weight_epsilon = self.weight_epsilon.cuda()\n",
    "            bias_epsilon   = self.bias_epsilon.cuda()\n",
    "        else:\n",
    "            weight_epsilon = self.weight_epsilon\n",
    "            bias_epsilon   = self.bias_epsilon\n",
    "\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(Variable(weight_epsilon))\n",
    "            bias   = self.bias_mu   + self.bias_sigma.mul(Variable(bias_epsilon))\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias   = self.bias_mu\n",
    "\n",
    "        return F.linear(x, weight, bias)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n",
    "\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))\n",
    "\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in  = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tcVnVeb2Poj6"
   },
   "outputs": [],
   "source": [
    "#code from openai\n",
    "#https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient `reduce`\n",
    "               operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the\n",
    "               array.\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must for a mathematical group together with the set of\n",
    "            possible values for array elements.\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n",
    "        for i in idxes:\n",
    "            data = self._storage[i]\n",
    "            obs_t, action, reward, obs_tp1, done = data\n",
    "            obses_t.append(np.array(obs_t, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            obses_tp1.append(np.array(obs_tp1, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n",
    "        return self._encode_sample(idxes)\n",
    "\n",
    "\n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(self, size, alpha):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayBuffer, self).__init__(size)\n",
    "        assert alpha > 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def push(self, *args, **kwargs):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "        super(PrioritizedReplayBuffer, self).push(*args, **kwargs)\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            # TODO(szymon): should we ensure no repeats?\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "        assert beta > 0\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = np.array(weights)\n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return tuple(list(encoded_sample) + [weights, idxes])\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = priority ** self._alpha\n",
    "            self._it_min[idx] = priority ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, priority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_vFCeKo9Odu4"
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1Fmye2BEN-74"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KgvBgb-WPQaw",
    "outputId": "1113f758-fec6-4cf9-86dd-944846935fb7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_258/1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMd0mk0VN-74"
   },
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "cE0HRfqrN-74"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPzFLOH3N-75"
   },
   "source": [
    "<h3> Rainbow: Combining Improvements in Deep Reinforcement Learning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ra8B4q2PN-75"
   },
   "outputs": [],
   "source": [
    "class RainbowDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, num_atoms, Vmin, Vmax):\n",
    "        super(RainbowDQN, self).__init__()\n",
    "\n",
    "        self.num_inputs   = num_inputs\n",
    "        self.num_actions  = num_actions\n",
    "        self.num_atoms    = num_atoms\n",
    "        self.Vmin         = Vmin\n",
    "        self.Vmax         = Vmax\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, 32)\n",
    "        self.linear2 = nn.Linear(32, 64)\n",
    "\n",
    "        self.noisy_value1 = NoisyLinear(64, 64, use_cuda=USE_CUDA)\n",
    "        self.noisy_value2 = NoisyLinear(64, self.num_atoms, use_cuda=USE_CUDA)\n",
    "\n",
    "        self.noisy_advantage1 = NoisyLinear(64, 64, use_cuda=USE_CUDA)\n",
    "        self.noisy_advantage2 = NoisyLinear(64, self.num_atoms * self.num_actions, use_cuda=USE_CUDA)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "\n",
    "        value = F.relu(self.noisy_value1(x))\n",
    "        value = self.noisy_value2(value)\n",
    "\n",
    "        advantage = F.relu(self.noisy_advantage1(x))\n",
    "        advantage = self.noisy_advantage2(advantage)\n",
    "\n",
    "        value     = value.view(batch_size, 1, self.num_atoms)\n",
    "        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)\n",
    "\n",
    "        x = value + advantage - advantage.mean(1, keepdim=True)\n",
    "        x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.noisy_value1.reset_noise()\n",
    "        self.noisy_value2.reset_noise()\n",
    "        self.noisy_advantage1.reset_noise()\n",
    "        self.noisy_advantage2.reset_noise()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "          state = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "          dist = self.forward(state).data.cpu()\n",
    "          dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)\n",
    "          action = dist.sum(2).max(1)[1].numpy()[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GKFuJjcQN-75"
   },
   "outputs": [],
   "source": [
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "current_model = RainbowDQN(env.observation_space.shape[0], env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "target_model  = RainbowDQN(env.observation_space.shape[0], env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters(), 0.001)\n",
    "\n",
    "replay_buffer = PrioritizedReplayBuffer(100_000, 0.5) # ReplayBuffer(100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZSJ1VGtHN-76"
   },
   "outputs": [],
   "source": [
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "HphvKUeQN-76"
   },
   "outputs": [],
   "source": [
    "def projection_distribution(next_state, rewards, dones):\n",
    "    batch_size  = next_state.size(0)\n",
    "\n",
    "    delta_z = float(Vmax - Vmin) / (num_atoms - 1)\n",
    "    support = torch.linspace(Vmin, Vmax, num_atoms)\n",
    "\n",
    "    next_dist   = target_model(next_state).data.cpu() * support\n",
    "    next_action = next_dist.sum(2).max(1)[1]\n",
    "    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(next_dist.size(0), 1, next_dist.size(2))\n",
    "    next_dist   = next_dist.gather(1, next_action).squeeze(1)\n",
    "\n",
    "    rewards = rewards.unsqueeze(1).expand_as(next_dist)\n",
    "    dones   = dones.unsqueeze(1).expand_as(next_dist)\n",
    "    support = support.unsqueeze(0).expand_as(next_dist)\n",
    "\n",
    "    Tz = rewards + (1 - dones) * 0.99 * support\n",
    "    Tz = Tz.clamp(min=Vmin, max=Vmax)\n",
    "    b  = (Tz - Vmin) / delta_z\n",
    "    l  = b.floor().long()\n",
    "    u  = b.ceil().long()\n",
    "\n",
    "    offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size).long()\\\n",
    "                    .unsqueeze(1).expand(batch_size, num_atoms)\n",
    "\n",
    "    proj_dist = torch.zeros(next_dist.size())\n",
    "    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n",
    "    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n",
    "\n",
    "    return proj_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replay_buffer.sample(batch_size, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUWsgfPPN-76"
   },
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "HDukQuiYN-76"
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    if 'PrioritizedReplayBuffer' in repr(replay_buffer):\n",
    "        state, action, reward, next_state, done, weights, indexes = replay_buffer.sample(batch_size, 0.7)\n",
    "    else:\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = torch.FloatTensor(reward)\n",
    "    done       = torch.FloatTensor(np.float32(done))\n",
    "\n",
    "    proj_dist = projection_distribution(next_state, reward, done)\n",
    "\n",
    "    dist = current_model(state)\n",
    "    action = action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_atoms)\n",
    "    dist = dist.gather(1, action).squeeze(1)\n",
    "    dist.data.clamp_(0.01, 0.99)\n",
    "    loss = -(Variable(proj_dist) * dist.log()).sum(1)\n",
    "    loss  = loss.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    current_model.reset_noise()\n",
    "    target_model.reset_noise()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "IfGKES3PN-76"
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, mean_rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    if len(mean_rewards) > 0:\n",
    "        plt.title('frame %s. mean reward: %s' % (frame_idx, mean_rewards[-1]))\n",
    "    plt.plot(rewards, color='b')\n",
    "    plt.plot(mean_rewards, color='y')\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJvWRIW4N-77"
   },
   "source": [
    "<p><hr></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu9Jn7kSN-77"
   },
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "BEEolxiAN-77"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = np.random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.env.step(ac)\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip       = skip\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "def make_atari(env_id, render_mode=None):\n",
    "    env = gym.make(env_id, render_mode=render_mode)\n",
    "    # assert 'NoFrameskip' in env.spec.id\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    if episode_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, 4)\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to num_channels x weight x height\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "\n",
    "\n",
    "def wrap_pytorch(env):\n",
    "    return ImageToPyTorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "id": "AglA7agRN-77",
    "outputId": "0e357b4d-def9-4b9d-efb6-fac9a7aa50f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ALE/Boxing-v5'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env_id = \"ALE/Breakout-v5\" #PongNoFrameskip-v4\"\n",
    "\n",
    "# Result for game from https://colab.research.google.com/drive/1LO7mJBnkccfwlKUFX17rJONbf_tUEukC?usp=sharing#scrollTo=TBrlxe0OVW1O\n",
    "# Boxing\t71.8 (8.4)\n",
    "# Breakout\t401.2 (26.9)\n",
    "\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "env_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDG32a21QCO_",
    "outputId": "91122b5f-7bd9-43e1-b4a2-2ca4ca96533f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-vlad-rl-0/lib/python3.7/site-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  \"Core environment is written in old step API which returns one bool instead of two. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[  0,   0,   0, ...,   0,   0,   0],\n",
       "         [  0,   0,   0, ...,   0,   0,   0],\n",
       "         [  0,   0,   0, ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [132, 132, 132, ..., 132, 132, 132],\n",
       "         [132, 132, 132, ..., 132, 132, 132],\n",
       "         [132, 132, 132, ..., 132, 132, 132]]], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'lives': 0, 'episode_frame_number': 124, 'frame_number': 124})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "_BXeiUPBN-77"
   },
   "outputs": [],
   "source": [
    "class RainbowCnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, num_atoms, Vmin, Vmax):\n",
    "        super(RainbowCnnDQN, self).__init__()\n",
    "\n",
    "        self.input_shape   = input_shape\n",
    "        self.num_actions  = num_actions\n",
    "        self.num_atoms    = num_atoms\n",
    "        self.Vmin         = Vmin\n",
    "        self.Vmax         = Vmax\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.noisy_value1 = NoisyLinear(self.feature_size(), 256, use_cuda=USE_CUDA)\n",
    "        self.noisy_value2 = NoisyLinear(256, self.num_atoms, use_cuda=USE_CUDA)\n",
    "\n",
    "        self.noisy_advantage1 = NoisyLinear(self.feature_size(), 256, use_cuda=USE_CUDA)\n",
    "        self.noisy_advantage2 = NoisyLinear(256, self.num_atoms * self.num_actions, use_cuda=USE_CUDA)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x / 255.\n",
    "        x = self.features(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        value = F.relu(self.noisy_value1(x))\n",
    "        value = self.noisy_value2(value)\n",
    "\n",
    "        advantage = F.relu(self.noisy_advantage1(x))\n",
    "        advantage = self.noisy_advantage2(advantage)\n",
    "\n",
    "        value     = value.view(batch_size, 1, self.num_atoms)\n",
    "        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)\n",
    "\n",
    "        x = value + advantage - advantage.mean(1, keepdim=True)\n",
    "        x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.noisy_value1.reset_noise()\n",
    "        self.noisy_value2.reset_noise()\n",
    "        self.noisy_advantage1.reset_noise()\n",
    "        self.noisy_advantage2.reset_noise()\n",
    "\n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "          state = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "          dist = self.forward(state).data.cpu()\n",
    "          dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)\n",
    "          action = dist.sum(2).max(1)[1].numpy()[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "tqW8w9R8N-77"
   },
   "outputs": [],
   "source": [
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "learning_rate = 0.000025\n",
    "\n",
    "replay_initial = 1_000\n",
    "# replay_buffer  = ReplayBuffer(200_000\n",
    "replay_buffer = PrioritizedReplayBuffer(200_000, 0.8) # ReplayBuffer(100_000)\n",
    "num_frames = 2_000_000\n",
    "batch_size = 64\n",
    "gamma      = 0.99\n",
    "episode_reward = 0\n",
    "epsilon = 0.98\n",
    "epsilon_min = 0.1\n",
    "\n",
    "current_model = RainbowCnnDQN(env.observation_space.shape, env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "target_model  = RainbowCnnDQN(env.observation_space.shape, env.action_space.n, num_atoms, Vmin, Vmax)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters(), lr=learning_rate)\n",
    "update_target(current_model, target_model)\n",
    "\n",
    "# https://github.com/AdrianHsu/breakout-Deep-Q-Network\n",
    "# replay_initial = 1_000\n",
    "# replay_buffer  = ReplayBuffer(1_000_000)\n",
    "# num_frames = 2_000_000\n",
    "# batch_size = 32\n",
    "# gamma      = 0.99\n",
    "# episode_reward = 0\n",
    "# epsilon = 0.98\n",
    "# epsilon_min = 0.05\n",
    "\n",
    "path = '/content/drive/MyDrive/RL/Boxing/'\n",
    "path = ''\n",
    "env_id_path = env_id.replace(\"/\",\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torchvision tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(comment=\"Priotity_0.8,eps_0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "DEa0D5fxN-77",
    "outputId": "5a3f6d72-f48d-4cd1-f4c2-0036d14fed31",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/user/conda/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3454/3559468283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mreplay_initial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3454/3018322130.py\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'PrioritizedReplayBuffer'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "loss = None\n",
    "all_rewards = []\n",
    "mean_rewards = []\n",
    "best_reward = -50.0\n",
    "target_reward = 71.8\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    action = current_model.act(state)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "      epsilon *= epsilon\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        mean_rewards.append(np.mean(all_rewards[-100:]))\n",
    "        episode_reward = 0\n",
    "\n",
    "\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    if loss is not None and frame_idx % 100 == 0:\n",
    "        writer.add_scalar(\"Loss/train\",           loss.detach().cpu().numpy(), frame_idx)\n",
    "        writer.add_scalar(\"Episode reward/train\", episode_reward, frame_idx)\n",
    "        writer.add_scalar(\"Mean reward/train\",    np.mean(all_rewards[-100:]), frame_idx)\n",
    "        plot(frame_idx, all_rewards, mean_rewards, losses)\n",
    "\n",
    "    if np.mean(all_rewards[-100:]) > best_reward:\n",
    "      best_reward = np.mean(all_rewards[-100:])\n",
    "      torch.save(target_model.state_dict(), f\"{path}{env_id_path}-best.dat\")\n",
    "\n",
    "    if np.mean(all_rewards[-100:]) > target_reward:\n",
    "        break\n",
    "        \n",
    "    if frame_idx % 1000 == 0:\n",
    "        update_target(current_model, target_model)\n",
    "        \n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQDJTlHF-zY4"
   },
   "outputs": [],
   "source": [
    "torch.save(target_model.state_dict(), f\"{path}{env_id_path}-last.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ZaJCg5wcQy",
    "outputId": "b7bd1a33-7244-474a-ff8c-26f0f4701ddf"
   },
   "outputs": [],
   "source": [
    "target_model.load_state_dict(torch.load(f\"{path}{env_id_path}-best.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXiIUpgPkUW7",
    "outputId": "85447627-9bc1-4cbf-b62b-1ed66ea7c761"
   },
   "outputs": [],
   "source": [
    "env    = make_atari(env_id, render_mode=\"rgb_array\")\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "# import pybulletgym\n",
    "import gym\n",
    "# from gym.wrappers import Monitor\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "# env = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "# env = make_atari(\"PongNoFrameskip-v4\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "vid = VideoRecorder(env=env.unwrapped, path=f\"{path}vid.mp4\")\n",
    "\n",
    "while not done:\n",
    "    # action = current_model.forward(torch.tensor(observation))\n",
    "    # z = action.argmax(dim=-1)\n",
    "    # action = z.argmax(dim=-1)\n",
    "    action = target_model.act(state)\n",
    "    # action =  env.action_space.sample()\n",
    "    # print(action)\n",
    "    state, reward, done,  _ = env.step(action)\n",
    "    env.render(mode='rgb_array')\n",
    "    vid.capture_frame()\n",
    "    # env.render(mode='human')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiP723LC3UHv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfEOtyH2kafu"
   },
   "outputs": [],
   "source": [
    "# библиотеки и функции, которые потребуеются для показа видео\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def show_video(folder=\".\"):\n",
    "    mp4list = glob.glob(folder + '/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "FGHSrs0OkejN",
    "outputId": "2f9bfba6-0adc-46ae-d683-1aebccec835a"
   },
   "outputs": [],
   "source": [
    "show_video(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVDtf4pHhO3m",
    "outputId": "ac188b61-0d89-424d-fa4c-c4750565dd84"
   },
   "outputs": [],
   "source": [
    "# num_games = 100\n",
    "# total_rewards = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   for i in range(num_games):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     episode_reward = 0\n",
    "#     while not done:\n",
    "#         # action = current_model.forward(torch.tensor(observation))\n",
    "#         # z = action.argmax(dim=-1)\n",
    "#         # action = z.argmax(dim=-1)\n",
    "#         action = target_model.act(state)\n",
    "#         # action =  env.action_space.sample()\n",
    "#         # print(action)\n",
    "#         state, reward, done,  _ = env.step(action)\n",
    "#         episode_reward += reward\n",
    "#     total_rewards.append(episode_reward)\n",
    "# np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_bt0Y8W4uTo",
    "outputId": "e7fc2831-777e-4a3c-b270-c30fc5d41ab9"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, agent, episodes=5, seed=0):\n",
    "    set_seed(env, seed=seed)\n",
    "\n",
    "    returns = []\n",
    "    for _ in range(episodes):\n",
    "        done, state, total_reward = False, env.reset(), 0\n",
    "\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(agent.act(state))\n",
    "            total_reward += reward\n",
    "        returns.append(total_reward)\n",
    "    return np.mean(returns), np.std(returns)\n",
    "\n",
    "evaluate_policy(env, target_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suTvjZvu85OH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
